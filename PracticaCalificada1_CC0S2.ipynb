{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e35a97d0",
      "metadata": {
        "id": "e35a97d0"
      },
      "source": [
        "## Primera práctica calificada\n",
        "\n",
        "**Nombre y Apellidos:** Ricardo Leonardo, OLIVARES VENTURA\n",
        "\n",
        "**Código:** 20192002A\n",
        "\n",
        "### Reglas para la Prueba Calificada\n",
        "\n",
        "- Queda terminantemente prohibido el uso de herramientas como ChatGPT, WhatsApp, o cualquier herramienta similar durante la realización de esta prueba. El uso de estas herramientas, por cualquier motivo, resultará en la anulación inmediata de la evaluación.\n",
        "\n",
        "- Las respuestas deben presentarse con una explicación detallada, utilizando términos técnicos apropiados. La mera descripción sin el uso de terminología técnica, especialmente términos discutidos en clase, se considerará insuficiente y podrá resultar en que la respuesta sea marcada como incorrecta.\n",
        "\n",
        "\n",
        "- Cada estudiante debe presentar su propio trabajo. Los códigos iguales o muy parecidos entre sí serán considerados como una violación a la integridad académica, implicando una copia, y serán sancionados de acuerdo con las políticas de la universidad.\n",
        "\n",
        "- Todos los estudiantes deben subir sus repositorios de código a la plataforma del curso, según las instrucciones proporcionadas. La fecha y hora de la última actualización del repositorio serán consideradas como la hora de entrega.\n",
        "\n",
        "- La claridad, orden, y presentación general de las evaluaciones serán tomadas en cuenta en la calificación final. Se espera un nivel de profesionalismo en la documentación y presentación del código y las respuestas escritas.\n",
        "\n",
        "\n",
        "#### Instrucciones de entrega para la prueba calificada\n",
        "\n",
        "- Presenta la dirección de tu repositorio personal donde se encuentre este cuaderno con tus respuestas desarrolladas.\n",
        "- Todo cambio fuera de la hora y fecha del examen realizado dentro del repositorio no se tomará en cuenta y se procederá a anular la evaluación."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c6884a0",
      "metadata": {
        "id": "3c6884a0"
      },
      "source": [
        "1 . Dividir un bloque de texto en subunidades significativas es una parte esencial del procesamiento de texto. El texto se puede dividir en caracteres o palabras individuales o en algún punto intermedio. A continuación se muestra un enfoque muy básico que divide el texto utilizando espacios en blanco. Ya existe un defecto, ya que la última palabra `dog` tiene puntuación.\n",
        "\n",
        "```\n",
        "'The quick brown fox jumps over the lazy dog.'.split(' ')\n",
        "```\n",
        "\n",
        "Con los modelos Transformer, realizamos tokenizaciones de subpalabras y dividimos el texto mediante un tokenizador prediseñado. Esto ha sido entrenado en una gran cantidad de texto donde ha aprendido cuáles son palabras comunes y cuáles son menos comunes y podrían dividirse en partes (que a menudo parecen sílabas).\n",
        "\n",
        "Primero, carguemos uno para un modelo de Transformer común `distilgpt2`. Podemos cargarlo con el siguiente código. El modelo distilgpt2 es un modelo más pequeño basado en gpt2 que vimos en clase, que es un predecesor del modelo de lenguaje que sustenta ChatGPT.\n",
        "\n",
        "```\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
        "```\n",
        "\n",
        "El tokenizador tiene una función `tokenizer.tokenize`  que divide el texto, así como funciones como `tokenizer.vocab`, `tokenizer.encode`, `tokenizer.decode`. El tokenizador tiene muchos parámetros para brindar control adicional. Por ejemplo, a veces es necesario truncar cadenas muy largas (ya que existe un límite en la longitud de entrada a los modelos Transformer). Puedes utilizar la función `tokenizer.encode` para tokenizar una oración como \"Kelvingrove is a beautiful park in Glasgow.\" que puedes recortar a solo 5 tokens usando `truncation=True` y `max_length=5`.\n",
        "\n",
        "\n",
        "Revisa: https://huggingface.co/transformers/v3.0.2/model_doc/auto.html\n",
        "\n",
        "\n",
        "**Preguntas:**\n",
        "\n",
        "* Encuentra una palabra en inglés que sea tokenizada en 3, 4, 5 o incluso 6 subtokens con el tokenizador distilgpt2 (1 punto)\n",
        "* Escribe una función en Python  para tokenizar un texto y mostrar los tokens y sus IDs, también escribe una función que utilice el tokenizador `datificate/gpt2-small-spanish` para identificar palabras que se descomponen en 3, 4, 5 o 6 subtokens, lo cual es interesante para entender la granularidad del tokenizador (2 puntos).\n",
        "\n",
        "* Al tokenizar, utilizaremos el tokenizador con el parámetro `return_tensors='pt'`. Esto coloca los datos en el formato de un tensor PyTorch, que se utiliza como entrada para un modelo Transformer. PyTorch es una biblioteca comúnmente utilizada para el aprendizaje profundo y HuggingFace se basa en ella. No usaremos PyTorch directamente. Tokeniza: `\"A horse! a horse! my kingdom for a\"` (1 punto)\n",
        "\n",
        "* Implementar un script en Python que use AutoModelForCausalLM para cargar un modelo de lenguaje causal. El ejercicio consistirá en tokenizar un texto, pasarlo a un modelo Transformer, y luego analizar las probabilidades de los tokens generados para identificar el más probable. Revisa: https://huggingface.co/docs/transformers/tasks/language_modeling (3 puntos)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a9b98310",
      "metadata": {
        "id": "a9b98310"
      },
      "outputs": [],
      "source": [
        "## Tus respuestas\n",
        "# - Encuentra una palabra en inglés que sea tokenizada en 3, 4, 5 o incluso 6 subtokens con el tokenizador distilgpt2\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"We are in the Natural Language Process class\"\n",
        "\n",
        "word1 = tokenizer.tokenize(word)\n",
        "\n",
        "print(word1) # Replacing unknown tokens (blank space in this question) with the unk_token 'Ġ'\n",
        "print(tokenizer.decode(tokenizer.encode(word, truncation=True, max_length=3)))\n",
        "print(tokenizer.decode(tokenizer.encode(word, truncation=True, max_length=4)))\n",
        "print(tokenizer.decode(tokenizer.encode(word, truncation=True, max_length=5)))\n",
        "print(tokenizer.decode(tokenizer.encode(word, truncation=True, max_length=6)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPdKpHz0K-Jn",
        "outputId": "24b91311-e02d-4fe8-a9b4-24411b846fdd"
      },
      "id": "DPdKpHz0K-Jn",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['We', 'Ġare', 'Ġin', 'Ġthe', 'ĠNatural', 'ĠLanguage', 'ĠProcess', 'Ġclass']\n",
            "We are in\n",
            "We are in the\n",
            "We are in the Natural\n",
            "We are in the Natural Language\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Escribe una función en Python para tokenizar un texto y mostrar los tokens y sus IDs,\n",
        "# también escribe una función que utilice el tokenizador datificate/gpt2-small-spanish\n",
        "# para identificar palabras que se descomponen en 3, 4, 5 o 6 subtokens, lo cual es interesante\n",
        "# para entender la granularidad del tokenizador (2 puntos).\n",
        "\n",
        "word = \"We are in the Natural Language Process class\"\n",
        "\n",
        "def tokenizeAndShowIDs():\n",
        "  encodedWord = tokenizer.encode(word, truncation=False) # Encode and tokenize the word\n",
        "  tokenizedWord = tokenizer.tokenize(word)\n",
        "  for idx, x in enumerate(encodedWord):\n",
        "    print(\"Token:  \",encodedWord[idx], \"--->Id: \",tokenizedWord[idx])\n",
        "\n",
        "\n",
        "tokenizeAndShowIDs()\n",
        "\n",
        "# Tokenize with datificate/gpt2-small-spanish\n",
        "# We will use modification of the original word\n",
        "\n",
        "tokenizer2 = AutoTokenizer.from_pretrained('datificate/gpt2-small-spanish')\n",
        "\n",
        "words = [\"Pepito juega fútbol en el parque\", \"Juan estudia en la UNI\", \"Mi perro está dormido\",\"Hace calor hoy\", \"María está jugando sola.\"]\n",
        "\n",
        "targetLenghts = [3,4,5,6]\n",
        "\n",
        "def tokenizeWithGPT2SmallSpanish():\n",
        "  for i in words:\n",
        "    tokenizedWord = tokenizer2.tokenize(i)\n",
        "    print(i, \" se descompone en: \", len(tokenizedWord), \"tokens ===> \", tokenizedWord)\n",
        "  print(\"\")\n",
        "  print(\"----Mostrando las palabras que se tokenizan en 3, 4, 5 o 6 tokens----\")\n",
        "  print(\"\")\n",
        "  for i in words:\n",
        "    tokenizedWord = tokenizer2.tokenize(i)\n",
        "    if len(tokenizedWord) in targetLenghts :\n",
        "      print(i, \" sí se tokeniza en\", len(tokenizedWord), \" tokens\")\n",
        "\n",
        "tokenizeWithGPT2SmallSpanish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX1V9gdrLl7J",
        "outputId": "4985639e-9b84-4636-d78d-6e3254df8abb"
      },
      "id": "JX1V9gdrLl7J",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token:   1135 --->Id:  We\n",
            "Token:   389 --->Id:  Ġare\n",
            "Token:   287 --->Id:  Ġin\n",
            "Token:   262 --->Id:  Ġthe\n",
            "Token:   12068 --->Id:  ĠNatural\n",
            "Token:   15417 --->Id:  ĠLanguage\n",
            "Token:   10854 --->Id:  ĠProcess\n",
            "Token:   1398 --->Id:  Ġclass\n",
            "Pepito juega fútbol en el parque  se descompone en:  8 tokens ===>  ['P', 'ep', 'ito', 'Ġjuega', 'ĠfÃºtbol', 'Ġen', 'Ġel', 'Ġparque']\n",
            "Juan estudia en la UNI  se descompone en:  6 tokens ===>  ['Juan', 'Ġestudia', 'Ġen', 'Ġla', 'ĠUN', 'I']\n",
            "Mi perro está dormido  se descompone en:  4 tokens ===>  ['Mi', 'Ġperro', 'ĠestÃ¡', 'Ġdormido']\n",
            "Hace calor hoy  se descompone en:  3 tokens ===>  ['Hace', 'Ġcalor', 'Ġhoy']\n",
            "María está jugando sola.  se descompone en:  5 tokens ===>  ['MarÃŃa', 'ĠestÃ¡', 'Ġjugando', 'Ġsola', '.']\n",
            "\n",
            "----Mostrando las palabras que se tokenizan en 3, 4, 5 o 6 tokens----\n",
            "\n",
            "Juan estudia en la UNI  sí se tokeniza en 6  tokens\n",
            "Mi perro está dormido  sí se tokeniza en 4  tokens\n",
            "Hace calor hoy  sí se tokeniza en 3  tokens\n",
            "María está jugando sola.  sí se tokeniza en 5  tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Al tokenizar, utilizaremos el tokenizador con el parámetro return_tensors='pt'.\n",
        "# Esto coloca los datos en el formato de un tensor PyTorch, que se utiliza como entrada para un modelo Transformer.\n",
        "# PyTorch es una biblioteca comúnmente utilizada para el aprendizaje profundo y HuggingFace se basa en ella.\n",
        "#  No usaremos PyTorch directamente. Tokeniza: \"A horse! a horse! my kingdom for a\" (1 punto)\n",
        "\n",
        "word3 = \"A horse! a horse! my kingdom for a\"\n",
        "\n",
        "# Using distilgpt2\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
        "\n",
        "tokenizedWord3 = tokenizer.tokenize(word3, return_tensors='pt')\n",
        "print(tokenizedWord3)\n",
        "\n",
        "# Using datificate/gpt2-small-spanish\n",
        "tokenizer2 = AutoTokenizer.from_pretrained('datificate/gpt2-small-spanish')\n",
        "\n",
        "tokenizedWord3 = tokenizer2.tokenize(word3, return_tensors='pt')\n",
        "print(tokenizedWord3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml5B7DgDSFL6",
        "outputId": "1bf8c404-e077-4a46-ba4f-b1d3486f27a4"
      },
      "id": "Ml5B7DgDSFL6",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'Ġhorse', '!', 'Ġa', 'Ġhorse', '!', 'Ġmy', 'Ġkingdom', 'Ġfor', 'Ġa']\n",
            "['A', 'Ġhor', 'se', '!', 'Ġa', 'Ġhor', 'se', '!', 'Ġmy', 'Ġk', 'ing', 'dom', 'Ġfor', 'Ġa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementar un script en Python que use AutoModelForCausalLM para cargar un modelo de lenguaje causal.\n",
        "# El ejercicio consistirá en tokenizar un texto, pasarlo a un modelo Transformer, y luego analizar\n",
        "# las probabilidades de los tokens generados para identificar el más probable.\n",
        "# Revisa: https://huggingface.co/docs/transformers/tasks/language_modeling (3 puntos)\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "word = \"We are in the Natural Language Process Exam\"\n",
        "\n",
        "# Tokenizamos el prompt\n",
        "input_ids = tokenizer(word, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Send tokenized word to the transforme model (gpt2 pretrained)\n",
        "gen_tokens = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    temperature=0.9,\n",
        "    max_length=100,\n",
        ")\n",
        "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
        "\n",
        "print(gen_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4UUORKxSvQD",
        "outputId": "9d5ea07c-ea40-4404-f0a1-085c9645fca3"
      },
      "id": "q4UUORKxSvQD",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We are in the Natural Language Process Exam that is part of our normal preparation for our first language course on our first Language Series course in October, 2015. We're going to have two separate Language Series course to take as part of our Intermediate Intermediate Language Series this Fall, so we will both be in the Natural Language Process Exam. Our course will focus exclusively on English as a primary language. Our Language Series will be designed in a very natural, natural way. We're going to work with teachers in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e017711b",
      "metadata": {
        "id": "e017711b"
      },
      "source": [
        "2 . En estos ejercicios se trata de profundizar en los modelos CBOW y Skim-Gram visto en clase:\n",
        "\n",
        "* Implementa un algoritmo para descubrir todos los 2-skip-2-gramas de una oración dada (2 puntos)\n",
        "* Expresa la función de pérdida únicamente como función de las entradas y los pesos, después de eliminar las variables de la capa oculta. (1 punto)\n",
        "\n",
        "* Calcula los gradientes de la función de pérdida con respecto a los pesos en las capas de entrada y salida (2 puntos)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "3469828f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3469828f",
        "outputId": "3835b1bf-6e8f-4307-c7fe-24b98a143724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Todos los 2-skip-2-gramas\n",
            "[['It', 'shows'], ['It', ',', 'shows'], ['It', 'my', 'shows'], ['shows', ','], ['shows', 'my', ','], ['shows', 'dear', ','], [',', 'my'], [',', 'dear', 'my'], [',', 'Watson', 'my'], ['my', 'dear'], ['my', 'Watson', 'dear'], ['my', ',', 'dear'], ['dear', 'Watson'], ['dear', ',', 'Watson'], ['dear', 'that', 'Watson'], ['Watson', ','], ['Watson', 'that', ','], ['Watson', 'we', ','], [',', 'that'], [',', 'we', 'that'], [',', 'are', 'that'], ['that', 'we'], ['that', 'are', 'we'], ['that', 'dealing', 'we'], ['we', 'are'], ['we', 'dealing', 'are'], ['we', 'with', 'are'], ['are', 'dealing'], ['are', 'with', 'dealing'], ['are', 'an', 'dealing'], ['dealing', 'with'], ['dealing', 'an', 'with'], ['dealing', 'exceptionally', 'with'], ['with', 'an'], ['with', 'exceptionally', 'an'], ['with', 'astude', 'an'], ['an', 'exceptionally'], ['an', 'astude', 'exceptionally'], ['an', 'and', 'exceptionally'], ['exceptionally', 'astude'], ['exceptionally', 'and', 'astude'], ['exceptionally', 'dangerous', 'astude'], ['astude', 'and'], ['astude', 'dangerous', 'and'], ['astude', 'man', 'and'], ['and', 'dangerous'], ['and', 'man', 'dangerous'], ['dangerous', 'man']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "## Tu respuesta\n",
        "#Implementa un algoritmo para descubrir todos los 2-skip-2-gramas de una oración dada (2 puntos)\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "word = \"It shows, my dear Watson, that we are dealing with an exceptionally astude and dangerous man\"\n",
        "tokenizedWord = word_tokenize(word)\n",
        "\n",
        "def generate_skip_grams(sequence, n, k):\n",
        "    skip_grams = []\n",
        "    for i in range(len(sequence) - n + 1):\n",
        "        base_gram = sequence[i:i+n]\n",
        "        skip_grams.append(base_gram)\n",
        "        for skip in range(1, k+1):\n",
        "            for j in range(i+n, min(len(sequence), i+n+skip)):\n",
        "                for l in range(n-1):\n",
        "                    skip_gram = base_gram[:l+1] + [sequence[j]] + base_gram[l+1:]\n",
        "                    if skip_gram not in skip_grams:\n",
        "                        skip_grams.append(skip_gram)\n",
        "    return skip_grams\n",
        "\n",
        "\n",
        "print(\"Todos los 2-skip-2-gramas\")\n",
        "print(generate_skip_grams(tokenizedWord, 2, 2))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Expresa la función de pérdida únicamente como función de las entradas y los pesos,\n",
        "# después de eliminar las variables de la capa oculta. (1 punto)\n",
        "\n",
        "def loss_function(inputs, weights):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(inputs, weights, from_logits=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "aNnuP990YqLi"
      },
      "id": "aNnuP990YqLi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula los gradientes de la función de pérdida con respecto a los pesos\n",
        "# en las capas de entrada y salida (2 puntos)."
      ],
      "metadata": {
        "id": "XDQp1W68YtLu"
      },
      "id": "XDQp1W68YtLu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "37b0c6e6",
      "metadata": {
        "id": "37b0c6e6"
      },
      "source": [
        "3 . De la actividad de modelos de lenguaje realizada en clase, las técnicas de suavizado, como el suavizado de Laplace, a menudo se agregan a los modelos de lenguaje de n-gramas para manejar probabilidades de 0. ¿Cómo podrías modificar tu código para incluirlo?.\n",
        "Puedes tambien experimentar con el suavizado de Good-Turing que ajusta las cuentas de los n-gramas basándose en la frecuencia de frecuencias de n-gramas. Es especialmente útil para redistribuir la masa de probabilidad a n-gramas no observados (2 puntos)\n",
        "\n",
        "De la misma actividad en los modelos que exploramos anteriormente, utilizamos suavizado. ¿Qué sucede con los cálculos de perplejidad cuando no se aplica el suavizado?  A veces se utiliza el suavizado interpolado o de \"back-off\" en los modelos de lenguaje de n-gramas. Esta técnica calcula la probabilidad promedio ponderada de modelos con diferentes valores de `n`. Implementa esto. ¿Cómo afecta esto la perplejidad en el conjunto de pruebas retenido? ¿Qué pasa con la perplejidad sobre los datos de entrenamiento? (3 puntos)\n",
        "\n",
        "\n",
        "El término 'Naïve' en la clasificación por Naïve Bayes se refiere a la suposición de independencia e idéntica distribución. Extiende el clasificador Naïve Bayes utilizando el concepto de modelado de lenguaje bigrama. El nuevo modelo pierde el atributo 'Naïve'. ¿Puedes integrar características de bolsa de palabras en este modelo utilizando técnicas de suavizado? (3 puntos)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0559b314",
      "metadata": {
        "id": "0559b314"
      },
      "outputs": [],
      "source": [
        "## Tus respuestas\n",
        "\n",
        "# De la actividad de modelos de lenguaje realizada en clase, las técnicas de suavizado, como el suavizado de Laplace,\n",
        "# a menudo se agregan a los modelos de lenguaje de n-gramas para manejar probabilidades de 0.\n",
        "# ¿Cómo podrías modificar tu código para incluirlo?. Puedes tambien experimentar con el suavizado de Good-Turing\n",
        "# que ajusta las cuentas de los n-gramas basándose en la frecuencia de frecuencias de n-gramas. Es especialmente\n",
        "# útil para redistribuir la masa de probabilidad a n-gramas no observados (2 puntos)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# De la misma actividad en los modelos que exploramos anteriormente, utilizamos suavizado.\n",
        "# ¿Qué sucede con los cálculos de perplejidad cuando no se aplica el suavizado? A veces se utiliza el\n",
        "# suavizado interpolado o de \"back-off\" en los modelos de lenguaje de n-gramas. Esta técnica calcula\n",
        "# la probabilidad promedio ponderada de modelos con diferentes valores de n. Implementa esto. ¿Cómo\n",
        "# afecta esto la perplejidad en el conjunto de pruebas retenido? ¿Qué pasa con la perplejidad sobre los datos de entrenamiento? (3 puntos)\n",
        "\n"
      ],
      "metadata": {
        "id": "Z0BNqRUxlC8z"
      },
      "id": "Z0BNqRUxlC8z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c302556f",
      "metadata": {
        "id": "c302556f"
      },
      "outputs": [],
      "source": [
        "# El término 'Naïve' en la clasificación por Naïve Bayes se refiere a la suposición de independencia e\n",
        "# idéntica distribución. Extiende el clasificador Naïve Bayes utilizando el concepto de modelado de lenguaje bigrama.\n",
        "# El nuevo modelo pierde el atributo 'Naïve'. ¿Puedes integrar características de bolsa de palabras en este modelo\n",
        "# utilizando técnicas de suavizado? (3 puntos)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}